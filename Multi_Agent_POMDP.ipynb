{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Conv3D, MaxPooling3D, Activation, Flatten, concatenate, Input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import modifiedTB as tb\n",
    "from agent import Point\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "MIN_REWARD = -1.00\n",
    "\n",
    "SIZE = 20\n",
    "N_BUILDINGS = 50\n",
    "N_OBSTACLES = 10\n",
    "MAX_HEIGHT = SIZE - 2\n",
    "MOVE_PENALTY = 0.01\n",
    "COLLISION_PENALTY = 0.25\n",
    "GOAL_REWARD = 1.00\n",
    "ACTION_SPACE_SIZE = 6\n",
    "GROUND_PROX_PENALTY = 0.2\n",
    "\n",
    "TOTAL_TIME = 1000\n",
    "MAX_LAMBDA = 0.1\n",
    "N_HUBS = 2\n",
    "\n",
    "OBSERVATION_DIST = 1  # Change this: {1,2,3}\n",
    "HETERO_REWARD = True  # Change this: {T(only when BINARY_ENV==F),F}\n",
    "BINARY_ENV = False  # Change this: {T,F}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, filepath):\n",
    "        self.terrain_size = (1 + (2 * OBSERVATION_DIST)) ** 3\n",
    "        # Load architecture and weights from model file\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    \n",
    "    def get_qs(self, state):\n",
    "        with tf.device('/GPU:0'):\n",
    "            return self.model.predict(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P2PEnv:\n",
    "    def reset(self, model):\n",
    "        self.terrain_map = self.generate_terrain()\n",
    "        self.dep_times = self.generate_dep_times(TOTAL_TIME, MAX_LAMBDA)\n",
    "        self.DQNagent = DQNAgent(model)\n",
    "        \n",
    "        empty_blocks_iter = self.empty_blocks(self.terrain_map)\n",
    "        self.agents = []\n",
    "        self.drones_map = np.zeros((SIZE, SIZE, SIZE))\n",
    "        self.goals_map = np.zeros((SIZE, SIZE, SIZE))\n",
    "\n",
    "        self.dynamic_obs = [Point(*next(empty_blocks_iter)) for i in range(N_OBSTACLES)]\n",
    "        self.obstacles_map = np.zeros((SIZE, SIZE, SIZE))\n",
    "        for obs in self.dynamic_obs:\n",
    "            self.obstacles_map[tuple(obs.location())] = 1\n",
    "        \n",
    "        # Keep track of the drone's trajectory\n",
    "        self.paths = []\n",
    "        \n",
    "        # Keep track of the drone's rewards, and whether it is done\n",
    "        self.rewards = [0] * len(self.dep_times)\n",
    "        self.dones = [False] * len(self.dep_times)\n",
    "            \n",
    "        self.episode_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.episode_step += 1\n",
    "        all_done = len(self.agents) == len(self.dep_times)\n",
    "        \n",
    "        if self.episode_step in self.dep_times:\n",
    "            terrain = self.terrain_map == 1\n",
    "            obstacles = self.obstacles_map == 1\n",
    "            drones = self.drones_map == 1\n",
    "            goals = self.goals_map == 1\n",
    "            empty_blocks_iter = self.empty_blocks(terrain|obstacles|drones|goals)\n",
    "            \n",
    "            for entries in range(len([t for t in self.dep_times if t == self.episode_step])):\n",
    "                start = Point(*next(empty_blocks_iter))\n",
    "                end = Point(*next(empty_blocks_iter))\n",
    "                L1 = sum(abs(start.location() - end.location()))\n",
    "                while L1 != SIZE:\n",
    "                    start = Point(*next(empty_blocks_iter))\n",
    "                    end = Point(*next(empty_blocks_iter))\n",
    "                    L1 = sum(abs(start.location() - end.location()))\n",
    "                self.agents.append([start, end])\n",
    "                self.paths.append([self.agents[-1][0].location()])\n",
    "                self.drones_map[tuple(self.agents[-1][0].location())] = 1\n",
    "                self.goals_map[tuple(self.agents[-1][1].location())] = 1\n",
    "                    \n",
    "        terrain = self.terrain_map == 1\n",
    "        obstacles = self.obstacles_map == 1\n",
    "        drones = self.drones_map == 1\n",
    "        \n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            if self.dones[idx]:\n",
    "                continue\n",
    "            \n",
    "            all_done = False\n",
    "            \n",
    "            current_state = self.generate_state(agent)\n",
    "            if np.random.rand() < 0.95:\n",
    "                for i in range(ACTION_SPACE_SIZE):\n",
    "                    action = np.argsort(self.DQNagent.get_qs([np.array([current_state[:self.DQNagent.terrain_size]]), np.array([current_state[self.DQNagent.terrain_size:]])]))[-i-1]\n",
    "                    n = agent[0].copy().action(action)    \n",
    "                    n.within_bounds(SIZE, SIZE, SIZE)\n",
    "                    if (terrain|obstacles|drones).astype(int)[tuple(n.location())] != 1:\n",
    "                        break\n",
    "            else:\n",
    "                for action in np.random.permutation(ACTION_SPACE_SIZE):\n",
    "                    n = agent[0].copy().action(action)    \n",
    "                    n.within_bounds(SIZE, SIZE, SIZE)\n",
    "                    if (terrain|obstacles|drones).astype(int)[tuple(n.location())] != 1:\n",
    "                        break\n",
    "            \n",
    "            self.drones_map[tuple(agent[0].location())] = 0            \n",
    "            agent[0].action(action).within_bounds(SIZE,SIZE,SIZE)\n",
    "            self.drones_map[tuple(agent[0].location())] = 1\n",
    "            \n",
    "            drones = self.drones_map == 1\n",
    "            \n",
    "            self.paths[idx].append(agent[0].location())\n",
    "            \n",
    "            if agent[0] == agent[1]:\n",
    "                reward = GOAL_REWARD\n",
    "                done = True\n",
    "            else:\n",
    "                reward = (-MOVE_PENALTY) * (1.0 + (HETERO_REWARD * 2.0 * self.terrain_map[tuple(agent[0].location())]))\n",
    "                done = False\n",
    "            \n",
    "            self.dones[idx] = done\n",
    "            self.rewards[idx] += reward\n",
    "                        \n",
    "            if done:\n",
    "                self.drones_map[tuple(agent[0].location())] = 0\n",
    "                self.goals_map[tuple(agent[1].location())] = 0\n",
    "                drones = self.drones_map == 1\n",
    "                    \n",
    "        goals = self.goals_map == 1\n",
    "        \n",
    "        obstacles = self.obstacles_map == 1\n",
    "        \n",
    "        for obs in self.dynamic_obs:\n",
    "            while True:\n",
    "                n = obs.copy()\n",
    "                n.drift_heading = obs.drift_heading\n",
    "                if n.drift().within_bounds(SIZE, SIZE, SIZE):\n",
    "                    if (terrain|obstacles|drones|goals).astype(int)[tuple(n.location())] != 1:\n",
    "                        break\n",
    "                else:\n",
    "                    obs.drift_heading = 2 * np.random.rand(3) - 1\n",
    "                    continue\n",
    "            self.obstacles_map[tuple(obs.location())] = 0\n",
    "            obs.x, obs.y, obs.z = n.x, n.y, n.z\n",
    "            self.obstacles_map[tuple(obs.location())] = 1\n",
    "            obstacles = self.obstacles_map == 1\n",
    "        \n",
    "        if self.episode_step >= TOTAL_TIME + 100:\n",
    "            all_done = True\n",
    "\n",
    "        return all_done\n",
    "\n",
    "    def render(self, elev=60, azim=45, save=\"\"):\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_zlabel(\"z\")\n",
    "        ax.view_init(elev=elev, azim=azim)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        terrain = self.terrain_map==1\n",
    "        drones = self.drones_map==1\n",
    "        goals = self.goals_map==1\n",
    "        obstacles = self.obstacles_map==1\n",
    "        \n",
    "        voxelarr = terrain | drones | goals | obstacles\n",
    "        colors = np.empty(terrain.shape, dtype=object)\n",
    "        colors[terrain] = '#7A88CCC0'\n",
    "        colors[drones] = '#FFD65DC0'\n",
    "        colors[goals] = '#607D3BC0'\n",
    "        colors[obstacles] = '#FDA4BAC0'\n",
    "        ax.voxels(voxelarr, facecolors=colors, shade=True)\n",
    "        \n",
    "#         for action in range(len(self.path)-1):\n",
    "#             xline = np.linspace(self.path[action][0] + 0.5, self.path[action+1][0] + 0.5, 1000)\n",
    "#             yline = np.linspace(self.path[action][1] + 0.5, self.path[action+1][1] + 0.5, 1000)\n",
    "#             zline = np.linspace(self.path[action][2] + 0.5, self.path[action+1][2] + 0.5, 1000)\n",
    "#             ax.plot3D(xline, yline, zline, 'black')\n",
    "        \n",
    "        if save != \"\":\n",
    "            plt.savefig(save)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def generate_terrain(self):\n",
    "        terrain = np.zeros((SIZE, SIZE, SIZE))\n",
    "        \n",
    "        if GROUND_PROX_PENALTY and not BINARY_ENV:\n",
    "            for i in range(SIZE//2):\n",
    "                terrain[:,:,i] = GROUND_PROX_PENALTY * (SIZE // 2 - i)/(SIZE//2)\n",
    "        \n",
    "        for i in range(N_BUILDINGS):\n",
    "            while True:\n",
    "                # Generate random numbers in intervals of 0.5\n",
    "                x, y = np.random.randint(0, SIZE*2, 2) / 2\n",
    "                # Generate random building height\n",
    "                z = np.random.randint(0, MAX_HEIGHT)\n",
    "                # Check if existing buildings exist. If so, regenerate. Otherwise, keep building.\n",
    "                if np.all(terrain[math.floor(x):math.ceil(x)+1, math.floor(y):math.ceil(y)+1, 0:z] != 1):\n",
    "                    if not BINARY_ENV:\n",
    "                        terrain[math.floor(x)-1:math.ceil(x)+2, math.floor(y)-1:math.ceil(y)+2, 0:z+1] \\\n",
    "                                = terrain[math.floor(x)-1:math.ceil(x)+2, math.floor(y)-1:math.ceil(y)+2, 0:z+1].clip(min=0.5)\n",
    "                    terrain[math.floor(x):math.ceil(x)+1, math.floor(y):math.ceil(y)+1, 0:z].fill(1)\n",
    "                    break\n",
    "        return terrain\n",
    "    \n",
    "    def generate_state(self, agent):\n",
    "        x,y,z = tuple(agent[0].location())\n",
    "        # padded_terrain = np.pad(np.maximum(np.maximum(self.terrain_map, self.obstacles_map), self.drones_map), OBSERVATION_DIST, 'constant', constant_values = 1)\n",
    "        padded_terrain = np.pad(np.maximum(self.terrain_map, self.obstacles_map), OBSERVATION_DIST, 'constant', constant_values = 1)\n",
    "        s = padded_terrain[x:x+1+2*OBSERVATION_DIST,y:y+1+2*OBSERVATION_DIST,z:z+1+2*OBSERVATION_DIST].flatten()\n",
    "        return np.append(s, agent[0].vector(agent[1]))\n",
    "\n",
    "    def empty_blocks(self, occupied):           \n",
    "        empty_blocks = [[x,y,z] for x in range(SIZE) for y in range(SIZE) for z in range(SIZE) if occupied[x,y,z]!=1]\n",
    "        random.shuffle(empty_blocks)\n",
    "        return iter(empty_blocks)\n",
    "    \n",
    "    def generate_dep_times(self, T, max_lambda):\n",
    "        # Rate function = max_lambda * (1 - (t - T/2)^2/(T^2/4))\n",
    "        dep_times = []\n",
    "        t = 0\n",
    "        while True:\n",
    "            next_time = np.random.exponential(1/max_lambda)\n",
    "            t += next_time\n",
    "            if t > T:\n",
    "                break\n",
    "            if np.random.rand() <= (1 - (t - T/2)**2/(T**2/4)):\n",
    "                dep_times.append(t)\n",
    "\n",
    "        dep_times = [math.floor(dep_time) for dep_time in dep_times]\n",
    "        return dep_times   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = P2PEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for test in [(\"25\",20,50,10,0.1),\n",
    "         (\"26\",20,50,10,0.2),\n",
    "         (\"27\",20,50,10,0.5),\n",
    "         (\"28\",20,50,25,0.1),\n",
    "         (\"29\",20,50,25,0.2),\n",
    "         (\"30\",20,50,25,0.5),\n",
    "         (\"31\",20,100,10,0.1),\n",
    "         (\"32\",20,100,10,0.2),\n",
    "         (\"33\",20,100,10,0.5)]:\n",
    "    \n",
    "    \n",
    "    MODEL_NAME, SIZE, N_BUILDINGS, N_OBSTACLES, MAX_LAMBDA = test\n",
    "    \n",
    "    for i, model in enumerate([\n",
    "#                     \"13_10600____0.97max____0.87avg____0.12min.model\",\n",
    "#                  \"14_14600____0.99max____0.80avg___-0.43min.model\",\n",
    "#                  \"15_07200____0.99max____0.87avg____0.74min.model\",\n",
    "#                  \"16_02600____0.97max____0.73avg___-1.06min.model\",\n",
    "#                  \"17_03200____0.97max____0.65avg___-0.48min.model\",\n",
    "#                  \"18_14000____0.98max____0.84avg___-0.26min.model\",\n",
    "                 \"19_04400____0.96max____0.79avg___-0.30min.model\"\n",
    "#                  \"20_10000____1.00max____0.72avg___-0.29min.model\",\n",
    "#                  \"21_22400____0.98max____0.64avg___-0.43min.model\",\n",
    "#                  \"22_01200____0.58max___-0.58avg___-1.30min.model\",\n",
    "#                  \"23_11200____0.88max___-0.00avg___-1.00min.model\",\n",
    "#                  \"24_10400____0.94max____0.21avg___-1.06min.model\"\n",
    "    ]):\n",
    "        \n",
    "        OBSERVATION_DIST = (i % 3) + 1\n",
    "\n",
    "        env = P2PEnv()\n",
    "\n",
    "        step = 1\n",
    "\n",
    "        env.reset(\"models/\"+model)\n",
    "        \n",
    "        env.render(save=f\"visualisations/{MODEL_NAME}_{step}.png\")\n",
    "        \n",
    "        while not env.step():\n",
    "            step += 1\n",
    "            env.render(save=f\"visualisations/{MODEL_NAME}_{step}.png\")\n",
    "#             if step >0 and step %100 == 0:\n",
    "#                 print(\".\", end=\"\")\n",
    "                \n",
    "#         rewards = env.rewards\n",
    "#         lengths = [len(p) for p in env.paths]\n",
    "\n",
    "#         with open(f'Test_{model[:2]}-Model_{MODEL_NAME}.csv', 'w', newline='') as file:\n",
    "#             mywriter = csv.writer(file, delimiter=',')\n",
    "#             for i in range(len(rewards)):\n",
    "#                 mywriter.writerow((rewards[i], lengths[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Average Cost = 0.011809616974969602, Percentage Tangible = 0.8939647747529545\n",
      "Average Cost = 0.014004681367296375, Percentage Tangible = 0.8864617664436013\n",
      "Average Cost = 0.012610210454715031, Percentage Tangible = 0.9120276327593051\n",
      "Average Cost = 0.012435746195430038, Percentage Tangible = 0.8987496506745097\n",
      "Average Cost = 0.015573461023109053, Percentage Tangible = 0.8910432416012914\n",
      "Average Cost = 0.014165214235651173, Percentage Tangible = 0.8996848228948593\n",
      "Average Cost = 0.011722559997229374, Percentage Tangible = 0.9164027772944262\n",
      "Average Cost = 0.01485814666409296, Percentage Tangible = 0.9030693224414397\n",
      "Average Cost = 0.01217137774101886, Percentage Tangible = 0.8957598565201309\n",
      "Average Cost = 0.018877568781755904, Percentage Tangible = 0.8778672821478948\n",
      "Average Cost = 0.012298665699818386, Percentage Tangible = 0.9120184854916348\n",
      "Average Cost = 0.00932256739899901, Percentage Tangible = 0.9411690678586694\n",
      "26\n",
      "Average Cost = 0.012316518612653192, Percentage Tangible = 0.8955909927323981\n",
      "Average Cost = 0.013157043287479853, Percentage Tangible = 0.8952708742904074\n",
      "Average Cost = 0.013442455075938416, Percentage Tangible = 0.8989243357509555\n",
      "Average Cost = 0.013174670963467762, Percentage Tangible = 0.8945760216344658\n",
      "Average Cost = 0.0123591169596961, Percentage Tangible = 0.9155249703986517\n",
      "Average Cost = 0.010985033323777316, Percentage Tangible = 0.9025913883585623\n",
      "Average Cost = 0.012700014165102448, Percentage Tangible = 0.8987202496476924\n",
      "Average Cost = 0.015399187479574628, Percentage Tangible = 0.8871332173898773\n",
      "Average Cost = 0.01174301337636243, Percentage Tangible = 0.9029275422208561\n",
      "Average Cost = 0.01550761316571021, Percentage Tangible = 0.9083409451524637\n",
      "Average Cost = 0.012035055882158283, Percentage Tangible = 0.9174816544745412\n",
      "Average Cost = 0.008947257925746271, Percentage Tangible = 0.9424641344813321\n",
      "27\n",
      "Average Cost = 0.011963323233645013, Percentage Tangible = 0.896785625325376\n",
      "Average Cost = 0.013601525129950685, Percentage Tangible = 0.8955504214507277\n",
      "Average Cost = 0.01242667505277767, Percentage Tangible = 0.903984835283898\n",
      "Average Cost = 0.012296950682517852, Percentage Tangible = 0.9014209302443597\n",
      "Average Cost = 0.0143174920341766, Percentage Tangible = 0.9025123032036341\n",
      "Average Cost = 0.012716328188743804, Percentage Tangible = 0.8964068870727007\n",
      "Average Cost = 0.01244066810150337, Percentage Tangible = 0.9088448198546145\n",
      "Average Cost = 0.013628835453270404, Percentage Tangible = 0.8916795798240034\n",
      "Average Cost = 0.012283777676045148, Percentage Tangible = 0.8981003788540758\n",
      "Average Cost = 0.020302949111003273, Percentage Tangible = 0.9088184353692965\n",
      "Average Cost = 0.012703040733973554, Percentage Tangible = 0.9090473575299792\n",
      "Average Cost = 0.011479514897137165, Percentage Tangible = 0.9024332262788737\n",
      "28\n",
      "Average Cost = 0.01091694308315983, Percentage Tangible = 0.919711094670584\n",
      "Average Cost = 0.011043784856740332, Percentage Tangible = 0.9150450630739245\n",
      "Average Cost = 0.012076098009830156, Percentage Tangible = 0.917928213434935\n",
      "Average Cost = 0.012392276535886116, Percentage Tangible = 0.9129040390250334\n",
      "Average Cost = 0.01336882506145212, Percentage Tangible = 0.9075645296276723\n",
      "Average Cost = 0.013059686386461242, Percentage Tangible = 0.891059366817321\n",
      "Average Cost = 0.011359623436161283, Percentage Tangible = 0.9205621073318488\n",
      "Average Cost = 0.011993482311080904, Percentage Tangible = 0.9162954337932109\n",
      "Average Cost = 0.011882057412227838, Percentage Tangible = 0.8930005103673264\n",
      "Average Cost = 0.026310360611431827, Percentage Tangible = 0.9491729779820104\n",
      "Average Cost = 0.012544867457994678, Percentage Tangible = 0.8998918181244963\n",
      "Average Cost = 0.01202039672549112, Percentage Tangible = 0.8879889250269782\n",
      "29\n",
      "Average Cost = 0.012985990970321952, Percentage Tangible = 0.8897562529554268\n",
      "Average Cost = 0.013339257462585154, Percentage Tangible = 0.8829544804793482\n",
      "Average Cost = 0.012048532597316166, Percentage Tangible = 0.9024992210940134\n",
      "Average Cost = 0.012534366970216444, Percentage Tangible = 0.8977482292444864\n",
      "Average Cost = 0.013824339539035554, Percentage Tangible = 0.9059558573446389\n",
      "Average Cost = 0.012800821567809935, Percentage Tangible = 0.8890416058592107\n",
      "Average Cost = 0.011116662388722259, Percentage Tangible = 0.9198782721737003\n",
      "Average Cost = 0.012260011793873007, Percentage Tangible = 0.9057575594018032\n",
      "Average Cost = 0.013506214740329921, Percentage Tangible = 0.8870664212595972\n",
      "Average Cost = 0.014957433389883406, Percentage Tangible = 0.895075206340741\n",
      "Average Cost = 0.01214288659221899, Percentage Tangible = 0.9211448564502906\n",
      "Average Cost = 0.011520388937543473, Percentage Tangible = 0.8983612433869275\n",
      "30\n",
      "Average Cost = 0.01250461486052457, Percentage Tangible = 0.9002218970473554\n",
      "Average Cost = 0.013188306661901324, Percentage Tangible = 0.8885642673012856\n",
      "Average Cost = 0.012302153299196239, Percentage Tangible = 0.9125508995515543\n",
      "Average Cost = 0.01362036380104087, Percentage Tangible = 0.9044276185765258\n",
      "Average Cost = 0.013314168322572768, Percentage Tangible = 0.914560342772851\n",
      "Average Cost = 0.01237690716103227, Percentage Tangible = 0.8902616927956117\n",
      "Average Cost = 0.012376270494455301, Percentage Tangible = 0.9120021852894936\n",
      "Average Cost = 0.013220844642068838, Percentage Tangible = 0.8959133164261485\n",
      "Average Cost = 0.012097276923059064, Percentage Tangible = 0.8952740381962428\n",
      "Average Cost = 0.015889405169362037, Percentage Tangible = 0.9059035928244987\n",
      "Average Cost = 0.011134690583692135, Percentage Tangible = 0.9323881661916223\n",
      "Average Cost = 0.012736964095864996, Percentage Tangible = 0.8868081485802455\n"
     ]
    }
   ],
   "source": [
    "with open(f'temp.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "\n",
    "    for MODEL_NAME in range(25,31):\n",
    "        print(MODEL_NAME)\n",
    "        store = []\n",
    "        average = [[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "        for agent in range(13,25):\n",
    "\n",
    "            rewards = []\n",
    "            lengths = []\n",
    "            with open(f'Test_{agent}-Model_{MODEL_NAME}.csv') as file:\n",
    "                myreader = csv.reader(file)\n",
    "                for i in myreader:\n",
    "                    rewards.append(float(i[0]))\n",
    "                    lengths.append(float(i[1]))\n",
    "\n",
    "            ELECTRIC_COST = 0.172625\n",
    "            VOR_HR = 1.308\n",
    "            DELTA_X_M = 2\n",
    "            DELTA_T_S = 0.2\n",
    "            VELOCITY = DELTA_X_M/DELTA_T_S\n",
    "\n",
    "            import statistics\n",
    "\n",
    "            avg_distance = statistics.mean([l for l in lengths if l < 101]) * DELTA_X_M\n",
    "            standard_dev = statistics.stdev([l for l in lengths if l < 101]) * DELTA_X_M\n",
    "\n",
    "            # print(f\"Average Time = {avg_distance / 10}\")\n",
    "            # print(f\"Std Dev Time = {standard_dev / 10}\")\n",
    "\n",
    "            avg_cost_t = ELECTRIC_COST / 1000 * (avg_distance)\n",
    "            # print(f\"Average Tangible Cost = {avg_cost}\")\n",
    "            avg_cost = avg_cost_t + standard_dev / VELOCITY * (VOR_HR / 3600)\n",
    "            print(f\"Average Cost = {avg_cost}, Percentage Tangible = {avg_cost_t/avg_cost}\")\n",
    "\n",
    "            store.append(str(avg_cost)[:7])\n",
    "\n",
    "        for x in range(4):\n",
    "            for y in range(3):\n",
    "                average[y][x] = store[x*3+y]\n",
    "\n",
    "        for z in average:\n",
    "            mywriter.writerow(z)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HASEnv:\n",
    "    def reset(self, model):\n",
    "        self.terrain_map, self.hubs = self.generate_terrain()\n",
    "        self.dep_times = self.generate_dep_times(TOTAL_TIME, MAX_LAMBDA)\n",
    "        self.DQNagent = DQNAgent(model)\n",
    "        \n",
    "        empty_blocks_iter = self.empty_blocks(self.terrain_map)\n",
    "        self.agents = []\n",
    "        self.drones_map = np.zeros((SIZE, SIZE, SIZE))\n",
    "        self.goals_map = np.zeros((SIZE, SIZE, SIZE))\n",
    "\n",
    "        self.dynamic_obs = [Point(*next(empty_blocks_iter)) for i in range(N_OBSTACLES)]\n",
    "        self.obstacles_map = np.zeros((SIZE, SIZE, SIZE))\n",
    "        for obs in self.dynamic_obs:\n",
    "            self.obstacles_map[tuple(obs.location())] = 1\n",
    "        \n",
    "        # Keep track of the drone's trajectory\n",
    "        self.paths = []\n",
    "        \n",
    "        # Keep track of the drone's rewards, and whether it is done\n",
    "        self.rewards = [0] * len(self.dep_times)\n",
    "        self.dones = [False] * len(self.dep_times)\n",
    "            \n",
    "        self.episode_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.episode_step += 1\n",
    "        all_done = len(self.agents) == len(self.dep_times)\n",
    "        \n",
    "        if self.episode_step in self.dep_times:\n",
    "            terrain = self.terrain_map == 1\n",
    "            obstacles = self.obstacles_map == 1\n",
    "            drones = self.drones_map == 1\n",
    "            goals = self.goals_map == 1\n",
    "            empty_blocks_iter = self.empty_blocks(terrain|obstacles|drones|goals)\n",
    "                        \n",
    "            for start_ in [self.hubs[i] for i in np.random.permutation(N_HUBS)][:len([t for t in self.dep_times if t == self.episode_step])]:\n",
    "                start = Point(*start_, 0)\n",
    "                end = Point(*next(empty_blocks_iter))\n",
    "                L1 = sum(abs(start.location() - end.location()))\n",
    "                while L1 != SIZE:\n",
    "                    end = Point(*next(empty_blocks_iter))\n",
    "                    L1 = sum(abs(start.location() - end.location()))\n",
    "                self.agents.append([start, end])\n",
    "                self.paths.append([self.agents[-1][0].location()])\n",
    "                self.drones_map[tuple(self.agents[-1][0].location())] = 1\n",
    "                self.goals_map[tuple(self.agents[-1][1].location())] = 1\n",
    "            \n",
    "        returns = []\n",
    "        \n",
    "        terrain = self.terrain_map == 1\n",
    "        obstacles = self.obstacles_map == 1\n",
    "        drones = self.drones_map == 1\n",
    "        \n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            if self.dones[idx]:\n",
    "                continue\n",
    "            \n",
    "            all_done = False\n",
    "            \n",
    "            current_state = self.generate_state(agent)\n",
    "            if np.random.rand() < 0.95:\n",
    "                for i in range(ACTION_SPACE_SIZE):\n",
    "                    action = np.argsort(self.DQNagent.get_qs([np.array([current_state[:self.DQNagent.terrain_size]]), np.array([current_state[self.DQNagent.terrain_size:]])]))[-i-1]\n",
    "                    n = agent[0].copy().action(action)    \n",
    "                    n.within_bounds(SIZE, SIZE, SIZE)\n",
    "                    if (terrain|obstacles|drones).astype(int)[tuple(n.location())] != 1:\n",
    "                        break\n",
    "            else:\n",
    "                for action in np.random.permutation(ACTION_SPACE_SIZE):\n",
    "                    n = agent[0].copy().action(action)    \n",
    "                    n.within_bounds(SIZE, SIZE, SIZE)\n",
    "                    if (terrain|obstacles|drones).astype(int)[tuple(n.location())] != 1:\n",
    "                        break\n",
    "            \n",
    "            self.drones_map[tuple(agent[0].location())] = 0            \n",
    "            agent[0].action(action).within_bounds(SIZE,SIZE,SIZE)\n",
    "            self.drones_map[tuple(agent[0].location())] = 1\n",
    "            \n",
    "            drones = self.drones_map == 1\n",
    "            \n",
    "            self.paths[idx].append(agent[0].location())\n",
    "            \n",
    "            if agent[0] == agent[1]:\n",
    "                reward = GOAL_REWARD\n",
    "                done = True\n",
    "            else:\n",
    "                reward = (-MOVE_PENALTY) * (1.0 + (HETERO_REWARD * 2.0 * self.terrain_map[tuple(agent[0].location())]))\n",
    "                done = False\n",
    "            \n",
    "            self.dones[idx] = done\n",
    "            self.rewards[idx] += reward\n",
    "                        \n",
    "            if done:\n",
    "                self.drones_map[tuple(agent[0].location())] = 0\n",
    "                self.goals_map[tuple(agent[1].location())] = 0\n",
    "                drones = self.drones_map == 1\n",
    "                    \n",
    "        goals = self.goals_map == 1\n",
    "        \n",
    "        obstacles = self.obstacles_map == 1\n",
    "        \n",
    "        for obs in self.dynamic_obs:\n",
    "            while True:\n",
    "                n = obs.copy()\n",
    "                n.drift_heading = obs.drift_heading\n",
    "                if n.drift().within_bounds(SIZE, SIZE, SIZE):\n",
    "                    if (terrain|obstacles|drones|goals).astype(int)[tuple(n.location())] != 1:\n",
    "                        break\n",
    "                else:\n",
    "                    obs.drift_heading = 2 * np.random.rand(3) - 1\n",
    "                    continue\n",
    "            self.obstacles_map[tuple(obs.location())] = 0\n",
    "            obs.x, obs.y, obs.z = n.x, n.y, n.z\n",
    "            self.obstacles_map[tuple(obs.location())] = 1\n",
    "            obstacles = self.obstacles_map == 1\n",
    "        \n",
    "        if self.episode_step >= TOTAL_TIME + 100:\n",
    "            all_done = True\n",
    "\n",
    "        return all_done\n",
    "\n",
    "    def render(self, elev=60, azim=45, save=\"\"):\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_zlabel(\"z\")\n",
    "        ax.view_init(elev=elev, azim=azim)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        terrain = self.terrain_map==1\n",
    "        drones = self.drones_map==1\n",
    "        goals = self.goals_map==1\n",
    "        obstacles = self.obstacles_map==1\n",
    "        \n",
    "        voxelarr = terrain | drones | goals | obstacles\n",
    "        colors = np.empty(terrain.shape, dtype=object)\n",
    "        colors[terrain] = '#7A88CCC0'\n",
    "        colors[drones] = '#FFD65DC0'\n",
    "        colors[goals] = '#607D3BC0'\n",
    "        colors[obstacles] = '#FDA4BAC0'\n",
    "        ax.voxels(voxelarr, facecolors=colors, shade=True)\n",
    "        \n",
    "#         for action in range(len(self.path)-1):\n",
    "#             xline = np.linspace(self.path[action][0] + 0.5, self.path[action+1][0] + 0.5, 1000)\n",
    "#             yline = np.linspace(self.path[action][1] + 0.5, self.path[action+1][1] + 0.5, 1000)\n",
    "#             zline = np.linspace(self.path[action][2] + 0.5, self.path[action+1][2] + 0.5, 1000)\n",
    "#             ax.plot3D(xline, yline, zline, 'black')\n",
    "        \n",
    "        if save != \"\":\n",
    "            plt.savefig(save)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def generate_terrain(self):\n",
    "        terrain = np.zeros((SIZE, SIZE, SIZE))\n",
    "        hubs = []\n",
    "        \n",
    "        if GROUND_PROX_PENALTY and not BINARY_ENV:\n",
    "            for i in range(SIZE//2):\n",
    "                terrain[:,:,i] = GROUND_PROX_PENALTY * (SIZE // 2 - i)/(SIZE//2)\n",
    "                \n",
    "        for i in range(N_HUBS):\n",
    "            while True:\n",
    "                x, y = np.random.normal((SIZE-1)/2, SIZE//10, 2)\n",
    "                x, y = int(x), int(y)\n",
    "                if terrain[x,y,0] != 1:\n",
    "                    terrain[x,y,0] = 1\n",
    "                    hubs.append((x,y))\n",
    "                    break\n",
    "        \n",
    "        for i in range(N_BUILDINGS):\n",
    "            while True:\n",
    "                # Generate random numbers in intervals of 0.5\n",
    "                x, y = np.random.randint(0, SIZE*2, 2) / 2\n",
    "                # Generate random building height\n",
    "                z = np.random.randint(0, MAX_HEIGHT)\n",
    "                # Check if existing buildings exist. If so, regenerate. Otherwise, keep building.\n",
    "                if np.all(terrain[math.floor(x):math.ceil(x)+1, math.floor(y):math.ceil(y)+1, 0:z] != 1):\n",
    "                    if not BINARY_ENV:\n",
    "                        terrain[math.floor(x)-1:math.ceil(x)+2, math.floor(y)-1:math.ceil(y)+2, 0:z+1] \\\n",
    "                                = terrain[math.floor(x)-1:math.ceil(x)+2, math.floor(y)-1:math.ceil(y)+2, 0:z+1].clip(min=0.5)\n",
    "                    terrain[math.floor(x):math.ceil(x)+1, math.floor(y):math.ceil(y)+1, 0:z].fill(1)\n",
    "                    break\n",
    "        return terrain, hubs\n",
    "    \n",
    "    def generate_state(self, agent):\n",
    "        x,y,z = tuple(agent[0].location())\n",
    "        # padded_terrain = np.pad(np.maximum(np.maximum(self.terrain_map, self.obstacles_map), self.drones_map), OBSERVATION_DIST, 'constant', constant_values = 1)\n",
    "        padded_terrain = np.pad(np.maximum(self.terrain_map, self.obstacles_map), OBSERVATION_DIST, 'constant', constant_values = 1)\n",
    "        s = padded_terrain[x:x+1+2*OBSERVATION_DIST,y:y+1+2*OBSERVATION_DIST,z:z+1+2*OBSERVATION_DIST].flatten()\n",
    "        return np.append(s, agent[0].vector(agent[1]))\n",
    "\n",
    "    def empty_blocks(self, occupied):           \n",
    "        empty_blocks = [[x,y,z] for x in range(SIZE) for y in range(SIZE) for z in range(SIZE) if occupied[x,y,z]!=1]\n",
    "        random.shuffle(empty_blocks)\n",
    "        return iter(empty_blocks)\n",
    "    \n",
    "    def generate_dep_times(self, T, max_lambda):\n",
    "        # Rate function = max_lambda * (1 - (t - T/2)^2/(T^2/4))\n",
    "        dep_times = []\n",
    "        t = 0\n",
    "        while True:\n",
    "            next_time = np.random.exponential(1/max_lambda)\n",
    "            t += next_time\n",
    "            if t > T:\n",
    "                break\n",
    "            if np.random.rand() <= (1 - (t - T/2)**2/(T**2/4)):\n",
    "                dep_times.append(t)\n",
    "\n",
    "        dep_times = [math.floor(dep_time) for dep_time in dep_times]\n",
    "        return dep_times   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "SIZE, N_BUILDINGS, N_OBSTACLES = 20, 50, 25\n",
    "\n",
    "for test in [(\"35\",2,0.2),\n",
    "         (\"36\",2,0.5),\n",
    "         (\"37\",5,0.1),\n",
    "         (\"38\",5,0.2),\n",
    "         (\"39\",5,0.5),\n",
    "         (\"40\",10,0.1),\n",
    "         (\"41\",10,0.2),\n",
    "         (\"42\",10,0.5)]:\n",
    "    \n",
    "    \n",
    "    MODEL_NAME, N_HUBS, MAX_LAMBDA = test\n",
    "    \n",
    "    for i, model in enumerate([\n",
    "#                 \"13_10600____0.97max____0.87avg____0.12min.model\",\n",
    "#                  \"14_14600____0.99max____0.80avg___-0.43min.model\",\n",
    "#                  \"15_07200____0.99max____0.87avg____0.74min.model\",\n",
    "#                  \"16_02600____0.97max____0.73avg___-1.06min.model\",\n",
    "#                  \"17_03200____0.97max____0.65avg___-0.48min.model\",\n",
    "#                  \"18_14000____0.98max____0.84avg___-0.26min.model\",\n",
    "                 \"19_04400____0.96max____0.79avg___-0.30min.model\"\n",
    "#                  \"20_10000____1.00max____0.72avg___-0.29min.model\",\n",
    "#                  \"21_22400____0.98max____0.64avg___-0.43min.model\",\n",
    "#                  \"22_01200____0.58max___-0.58avg___-1.30min.model\",\n",
    "#                  \"23_11200____0.88max___-0.00avg___-1.00min.model\",\n",
    "#                  \"24_10400____0.94max____0.21avg___-1.06min.model\"\n",
    "    ]):\n",
    "        \n",
    "        OBSERVATION_DIST = (i % 3) + 1\n",
    "\n",
    "        env = HASEnv()\n",
    "#         print(f'\\nTest_{model[:2]}-Model_{MODEL_NAME}.csv ', end='')\n",
    "\n",
    "        step = 1\n",
    "\n",
    "        env.reset(\"models/\"+model)\n",
    "        \n",
    "        env.render(save=f\"visualisations/{MODEL_NAME}_{step}.png\")\n",
    "        \n",
    "        while not env.step():\n",
    "            step += 1\n",
    "            env.render(save=f\"visualisations/{MODEL_NAME}_{step}.png\")\n",
    "            \n",
    "            \n",
    "#             if step >0 and step %100 == 0:\n",
    "#                 print(\".\", end=\"\")\n",
    "#         rewards = env.rewards\n",
    "#         lengths = [len(p) for p in env.paths]\n",
    "\n",
    "#         with open(f'Test_{model[:2]}-Model_{MODEL_NAME}.csv', 'w', newline='') as file:\n",
    "#             mywriter = csv.writer(file, delimiter=',')\n",
    "#             for j in range(min(len(rewards), len(lengths))):\n",
    "#                 mywriter.writerow((rewards[j], lengths[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
